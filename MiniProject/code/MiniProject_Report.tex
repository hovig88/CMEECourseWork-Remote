\documentclass[11pt]{article}
\usepackage{setspace}
\usepackage{lineno}
\usepackage{indentfirst}

\doublespacing

% Word Count %
\newcommand\wordcount{\input{texcount2.sum}}

\begin{document}


	\begin{titlepage}
		\centering
		\vspace*{\fill}
		Imperal College London\\
		\textbf{An Information-Theoretic Approach to Model Selection}\\
		Department of Life Sciences\\
		\hfill \break
		\hfill \break
		Hovig Artinian\\
		MSc CMEE\\
		March 6, 2020\\
		Word count: \wordcount
		\vspace*{\fill}
	\end{titlepage}


	\tableofcontents
	
	\newpage
	\begin{linenumbers}
	\section{Introduction}

	Modelling nature has been the general interest in the field of ecology for over a century \cite{Kingsland1995}. A crucial part of this process involves model selection. The basic approach is the null hypothesis testing, where biological inferences are made based on whether or not a suggested hypothesis is rejected \cite{Johnson2004}. This would be based on arbitrary criteria such as p-values, confidence intervals, and t-tests, that are set by statisticians as general rules of thumb. Amidst this mayhem, however, some scientists have started to shift their workflow towards more robust approaches that rely on advanced mathematical theories.
	\par In this project, the information-theoretic approach is used for model selection. This methodology is based on information theory \cite{Guiasu1977}.
	\par Preceding the model selection phase, however, is finding parameter estimates of models by fitting them to data. 7 models are used in this study: linear, quadratic, cubic, logistic \cite{Pearl1920}, gompertz \cite{Zwietering1990}, baranyi \cite{Baranyi1994}, and buchanan \cite{Buchanan1997}. The first three are phenomenological, which means their parameters have no biological significance. For the mechanistic models, however, 4 parameters are involved and are described in Table \ref{tab:parameters} below.
	
	\begin{table}[ht]
		\centering
		\begin{tabular}{|l|l|l|}
			\hline
			Parameter & Description              & Models included                       \\ \hline\hline
			$N_{max}$ & carrying capacity        & logistic, gompertz, baranyi, buchanan \\ \hline
			$N_0$     & initial abundance value  & logistic, gompertz, baranyi, buchanan \\ \hline
			$r_{max}$ & growth rate              & logistic, gompertz, baranyi, buchanan \\ \hline
			$t_{lag}$ & time taken for lag phase & gompertz, baranyi, buchanan           \\ \hline
		\end{tabular}
		\caption{Parameters involved in the mechanistic models used in this study with their corresponding descriptions.}
		\label{tab:parameters}
	\end{table}

	\par The Non-Linear Least Squares (NLLS) technique was performed for model fitting, which allows models to detect non-linear patterns found in data. Once parameter estimates are found, model selection is performed by calculating several information-theoretic criteria such as AIC, $AIC_c$, BIC, $R^2$, AIC differences, likelihood of models, Akaike weights, and evidence ratios.
	\par The main objective of this project is to show how sometimes finding a "best" model is not always ideal; rather, a multimodel inference approach would be optimal instead.

	\section{Materials \& Methods}
	\subsection{Data Preparation}
	The starting dataset consisted of 4387 samples. No missing abundance values were detected. However, negative values were present. The smallest value was largely negative and, hence, removed. To deal with the rest, while still minimizing the amount of data points lost, the smallest value was added to the whole data, and then removed (to avoid having zero as an abundance value). The end result was a dataset with 4385 values.
	Next, each species/temperature/medium/citation/replicate was grouped together, resulting in 305 unique IDs. Finally, the new dataset was saved to be used for data analysis.

	\subsection{Data Analysis}
	\subsubsection{Model Fitting}
	\par Non-linear least squares (NLLS) fitting was used to fit all 7 models to each unique group in the new dataset. 
	\par To work with this method, starting parameter values must first be provided. The better the starting values, the more precise the estimated parameter values will be.
	\par For phenomenological models, finding the starting values was straightforward (they were set to 1). In the case of mechanistic models, on the other hand, more computation was needed. The starting values of $N_{max}$ and $N_0$ were set to be the highest and lowest abundance values in the dataset, respectively. That of $r_{max}$ was less direct. A straight-line was fit to the first 50\% of the dataset, and its slope was assigned as the starting value of $r_{max}$. Lastly, the intersection point between the fitted tangent line and the horizontal line at y = $N_0$ was set to be the starting value of $t_{lag}$.
	\par Next, the actual fitting was performed, where residuals for each models to be fit were provided using the newly found starting values. For each model, if the fit converged, the estimated parameters were saved in a variable; otherwise, the estimated parameter values were set to 0. 
	\subsubsection{Model selection}
	\par For model selection, first, Akaike's Information Criterion (AIC) \cite{Akaike1973} was calculated for each model. The AIC value gives the quality of each model relative to the other models in the set used for fitting the data. Hence, the model with the lowest AIC score is preferred. For models where the sample size (n) to number of parameters (K) ratio was less than 40 (arbitrary suggestion), $AIC_c$ was calculated instead. It is known to be the second-order variant of AIC derived by Sugiura  in 1978 \cite{Sugiura1978}. The difference is in the bias-correction term added to the AIC value of the model. This makes sure that when sample size and number of parameters are close, the model gets heavily penalized.
	\par Next, AIC\footnote[1]{To avoid repetition, the term "AIC" will be used to mean both AIC and $AIC_c$. There is, however, a distinction between them, as explained, and the reader should bear that in mind.} differences ($\Delta_i$) were calculated to get a better idea of the empirical support provided by each model in the set; thus, obtaining a relative ranking of all the models in the set. The formula is given as follows:
	\begin{equation}
		\Delta_i = AIC_i - AIC_{min}
	\end{equation}
	where $AIC_i$ is the AIC value of the \emph{i}th model and $AIC_{min}$ is the AIC value of the "best" model in the set. The larger the difference, the less likely it is for that model to be the best. This likelihood was quantified and calculated by the following formula:
	\begin{equation}
		\mathcal{L}(g_i|x) \propto e^{-\frac{1}{2}\Delta_i}
	\end{equation}
	
	\par Contrary to $\Delta_i$, the larger the relative likelihood of a model, the better chances it has to be the preferred model.
	For better interpretation, the likelihood of the models were normalized, adding to 1. This normalized form is known as the Akaike weight, $w_i$ of a model:
	\begin{equation}
		w_i = \frac{e^{-\frac{1}{2}\Delta_i}}{\displaystyle\sum_{r=1}^{R} e^{-\frac{1}{2}\Delta_r}}
	\end{equation}
	where R is the number of models.
	
	\par Lastly, evidence ratios were calculated to show how reliable it is to believe that the best model is actually a good fit for the data. To put it in another way, evidence ratios reveal whether or not the best model in a set of models works best alone, or within a group consisting of one or more of the other models in the set. It can be found by simply calculating the ratio of Akaike weights:
	\begin{equation}
		Evidence\ ratio = \frac{w_1}{w_j}
	\end{equation}
	where $w_1$ is the Akaike weight of the model with the lowest AIC value, and $w_j$ is the Akaike weight of the \emph{j}th model.
	
	\subsection{Computing Tools}
	Several programming languages were used to create the different aspects of this project.
	
	\par \textbf{R} was used for: (1) data exploration/preparation - playing around with data in R is fast and intuitive, (2) plotting - really good packages that produce nice-looking plots, (3) finding information-theoretic criteria - since calculating statistical measures is fast and straighforward in R. Packages used: \emph{dplyr} \cite{Wickham2018} for data manipulation and \emph{ggplot2} \cite{Wickham2009} for plotting. \textbf{Python} was used to perform heavy computation, especially model fitting since the package for NLLS fitting in Python is much more robust than the one in R. Packages used: \emph{Pandas} \cite{McKinney2015} for using dataframes, \emph{NumPy} \cite{©Copyright2017NumPydevelopers.2017} for scientific and numeric computing, and \emph{LMFIT} \cite{Newville2014} for NLLS fitting. \textbf{\LaTeX} was used for typesetting. \textbf{Bash} was used to glue the R and Python scripts together. \textbf{Git} was used for version control of all codes/scripts/workflow.

	\section{Results}
	\subsection{Model Fitting}
	Model fitting convergence success rate was 100\% for 5 out of the 7 models used for NLLS fitting. The baranyi model converged 80.98\% of the time when fitted to all 305 groups, while the the logistic model did  not converge with over half the total number of groups in the dataset (44.92\%)
	
	\par Figure \label{} below gives a general idea of what some of the data look like and how the models were able to be overlaid.
	
	\par However, visualizing the plots is not a reliable way of determining good fits. Looking at the model selection criteria can be more helpful in this case, which is shown in the next section.
	
	\subsection{Model Selection}
	\subsubsection{Case 1}
	
	\section{Discussion}

	Although Akaike's Information Criterion is recognized as a major measure for selecting models, it has one major drawback: The AIC values lack intuitivity (easy to use and understand) despite higher values meaning less goodness-of-fit. For this purpose, Akaike weights come to hand for calculating the weights in a regime of several models. Additional measures can be derived, such as Δ(AIC) and relative likelihoods that demonstrate the probability of one model being in favor over the other.\\
	
	AIC:\\
	good thing: it accounts for overfitting - it has a penalty that increases as more parameters are added. Adding parameters will almost always improve the goodness of the fit.\\
	An individual AIC value, by itself, is not interpretable due to the unknown constant (interval scale). AIC is only comparative, relative to other AIC values in the model set; thus such differences $\Delta_i$ are very important and useful.
	
	It is important to note here that AIC values do not give any information about the goodness-of-fit of a model to the data. Rather, they show how each model performs relative to the other ones in the set. Hence, a single AIC value of a model has no meaning.
	
	\textbf{BOOK:}\\
	Ambivalence:
	\par The inability to ferret out a single best model is not a defect of AIC or any other selection criterion. Rather, it is an indication that the data are simply inadequate to reach such a strong inference. That is, the data are ambivalent concerning some effect or parametrization or structure.
	\par In such cases, all the models in the set can be used to make robust inferences: multimodel inference.\\

	\par The AIC differences ($\Delta_i$) and Akaike weights ($w_i$) are important in ranking and scaling the hypotheses, represented by models. The evidence ratios (e.g., $w_i$/$w_j$) help sharpen the evidence for or against the various alternative hypotheses. All of these values are easy to compute and simple to understand and interpret.\\

	\par The principle of parsimony provides a philosophical basis for model selection, K-L information provides an objective target based on deep theory, and AIC, $AIC_c$, $QAIC_c$, and TIC provide estimators of relative, expected K-L information. Objective model selection is rigorously based on these principles. These methods are applicable across a very wide range of scientific hypotheses and statistical models. We recommend presentation of log($\mathcal{L}$($\hat\theta$)), K, the appropriate information criterion (AIC, $AIC_c$, $QAIC_c$, or TIC), $\Delta_i$, and $w_i$ for various models in research papers to provide full information concerning the evidence for each of the models.\\

	\par \textbf{Do not mix null hypothesis testing with information-theoretic criteria:}\\
	Some authors state that the best model (say $g_1$) is \emph{significantly} better than another model (say $g_6$ based on a $\Delta$ value of 4-7. Alternatively, sometimes one sees that model $g_6$ is rejected relative to the best model. These statements are poor and misleading. It seems best not to associate the words significant or rejected with results under an information-theoretic paradigm. Questions concerning the strength of evidence for the models in the set are best addressed using the evidence ratio (Section 2.10), as well as an analysis of residuals, adjusted R2, and other model diagnostics or descriptive statistics.

	\section{Conclusion \& Future Work}
	studying the death phase %include a figure that has all 4 phases%
	
	\section{Acknowledgements}
	I would like to extend my gratitude to my fellow colleagues Pablo Lechon and Sam Turner for engaging in insightful discussions with me about the miniproject, as well as pulling an all-nighter the night before the submission deadline.
	
	\end{linenumbers}

	\bibliographystyle{unsrt}
    \bibliography{library}

\end{document}
