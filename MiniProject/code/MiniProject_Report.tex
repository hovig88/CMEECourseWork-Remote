\documentclass[11pt]{article}
\usepackage{setspace}
\usepackage{lineno}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{subfig}

\graphicspath{{../results/}}

\doublespacing

% Word Count %
\newcommand\wordcount{\input{texcount2.sum}}

\begin{document}


	\begin{titlepage}
		\centering
		\vspace*{\fill}
		Imperal College London\\
		\textbf{An Information-Theoretic Approach to Model Selection}\\
		Department of Life Sciences\\
		\hfill \break
		\hfill \break
		Hovig Artinian\\
		MSc CMEE\\
		Word count: \wordcount
		\vspace*{\fill}
	\end{titlepage}


	\tableofcontents
	
	\newpage
	\begin{linenumbers}
	\section{Introduction}

	Modelling nature has been the general interest in the field of ecology for over a century \cite{Kingsland1995}. A crucial part of this process involves model selection. The basic approach is the null hypothesis testing, where biological inferences are made based on whether or not a suggested hypothesis is rejected \cite{Johnson2004}. This would be based on arbitrary criteria such as p-values, confidence intervals, and t-tests, that are set by statisticians as general rules of thumb. Amidst this mayhem, however, some scientists have started to shift their workflow towards more robust approaches that rely on advanced mathematical theories.
	\par In this project, the information-theoretic approach is used for model selection. This methodology is based on information theory \cite{Guiasu1977}.
	\par Preceding the model selection phase, however, is finding parameter estimates of models by fitting them to data. 7 models are used in this study: linear, quadratic, cubic, logistic \cite{Pearl1920}, gompertz \cite{Zwietering1990}, baranyi \cite{Baranyi1994}, and buchanan \cite{Buchanan1997}. The first three are phenomenological, which means their parameters have no biological significance. For the mechanistic models, however, 4 parameters are involved and are described in Table \ref{tab:parameters} below.
	
	\begin{table}[ht]
		\centering
		\begin{tabular}{|l|l|l|}
			\hline
			Parameter & Description              & Models included                       \\ \hline\hline
			$N_{max}$ & carrying capacity        & logistic, gompertz, baranyi, buchanan \\ \hline
			$N_0$     & initial abundance value  & logistic, gompertz, baranyi, buchanan \\ \hline
			$r_{max}$ & growth rate              & logistic, gompertz, baranyi, buchanan \\ \hline
			$t_{lag}$ & time taken for lag phase & gompertz, baranyi, buchanan           \\ \hline
		\end{tabular}
		\caption{Parameters involved in the mechanistic models used in this study with their corresponding descriptions.}
		\label{tab:parameters}
	\end{table}

	\par The Non-Linear Least Squares (NLLS) technique was performed for model fitting, which allows models to detect non-linear patterns found in data. Once parameter estimates are found, model selection is performed by calculating several information-theoretic criteria such as AIC, $AIC_c$, BIC, $R^2$, AIC differences, likelihood of models, Akaike weights, and evidence ratios.
	\par The main objective of this project is to show how sometimes finding a "best" model is not always ideal; rather, a multimodel inference approach would be optimal instead.

	\section{Materials \& Methods}
	\subsection{Data Preparation}
	The starting dataset consisted of 4387 samples. No missing abundance values were detected. However, negative values were present. The smallest value was largely negative and, hence, removed. To deal with the rest, while still minimizing the amount of data points lost, the smallest value was added to the whole data, and then removed (to avoid having zero as an abundance value). The end result was a dataset with 4385 values.
	Next, each species/temperature/medium/citation/replicate was grouped together, resulting in 305 unique IDs. Finally, the new dataset was saved to be used for data analysis.

	\subsection{Data Analysis}
	\subsubsection{Model Fitting}
	\par Non-linear least squares (NLLS) fitting was used to fit all 7 models to each unique group in the new dataset. 
	\par To work with this method, starting parameter values must first be provided. The better the starting values, the more precise the estimated parameter values will be.
	\par For phenomenological models, finding the starting values was straightforward (they were set to 1). In the case of mechanistic models, on the other hand, more computation was needed. The starting values of $N_{max}$ and $N_0$ were set to be the highest and lowest abundance values in the dataset, respectively. That of $r_{max}$ was less direct. A straight-line was fit to the first 50\% of the dataset, and its slope was assigned as the starting value of $r_{max}$. Lastly, the intersection point between the fitted tangent line and the horizontal line at y = $N_0$ was set to be the starting value of $t_{lag}$.
	\par Next, the actual fitting was performed, where residuals for each models to be fit were provided using the newly found starting values. For each model, if the fit converged, the estimated parameters were saved in a variable; otherwise, the estimated parameter values were set to 0. 
	\subsubsection{Model selection}
	\par For model selection, first, Akaike's Information Criterion (AIC) \cite{Akaike1973} was calculated for each model. The AIC value gives the quality of each model relative to the other models in the set used for fitting the data. Hence, the model with the lowest AIC score is preferred. For models where the sample size (n) to number of parameters (K) ratio was less than 40 (arbitrary suggestion), $AIC_c$ was calculated instead. It is known to be the second-order variant of AIC derived by Sugiura  in 1978 \cite{Sugiura1978}. The difference is in the bias-correction term added to the AIC value of the model \cite{Anderson2002}. This makes sure that when sample size and number of parameters are close, the model gets heavily penalized.
	\par Next, AIC\footnote[1]{To avoid repetition, the term "AIC" will be used to mean both AIC and $AIC_c$. There is, however, a distinction between them, as explained, and the reader should bear that in mind.} differences ($\Delta_i$) were calculated to get a better idea of the empirical support provided by each model in the set; thus, obtaining a relative ranking of all the models in the set. The formula is given as follows:
	\begin{equation}
		\Delta_i = AIC_i - AIC_{min}
	\end{equation}
	where $AIC_i$ is the AIC value of the \emph{i}th model and $AIC_{min}$ is the AIC value of the "best" model in the set. The larger the difference, the less likely it is for that model to be the best. This likelihood was quantified and calculated by the following formula:
	\begin{equation}
		\mathcal{L}(g_i|x) \propto e^{-\frac{1}{2}\Delta_i}
	\end{equation}
	
	\par Contrary to $\Delta_i$, the larger the relative likelihood of a model, the better chances it has to be the preferred model.
	For better interpretation, the likelihood of the models were normalized, adding to 1. This normalized form is known as the Akaike weight, $w_i$ of a model:
	\begin{equation}
		w_i = \frac{e^{-\frac{1}{2}\Delta_i}}{\displaystyle\sum_{r=1}^{R} e^{-\frac{1}{2}\Delta_r}}
	\end{equation}
	where R is the number of models.
	
	\par Lastly, evidence ratios were calculated to show how reliable it is to believe that the best model is actually a good fit for the data \cite{Anderson2002}. To put it in another way, evidence ratios reveal whether or not the best model in a set of models works best alone, or within a group consisting of one or more of the other models in the set. It can be found by simply calculating the ratio of Akaike weights:
	\begin{equation}
		Evidence\ ratio = \frac{w_1}{w_j}
	\end{equation}
	where $w_1$ is the Akaike weight of the model with the lowest AIC value, and $w_j$ is the Akaike weight of the \emph{j}th model.
	
	\subsection{Computing Tools}
	Several programming languages were used to create the different aspects of this project.
	
	\par \textbf{R} \cite{RDevelopmentCoreTeam2011} was used for: (1) data exploration/preparation - playing around with data in R is fast and intuitive, (2) plotting - really good packages that produce nice-looking plots, (3) finding information-theoretic criteria - since calculating statistical measures is fast and straighforward in R. Packages used: \emph{dplyr} \cite{Wickham2018} for data manipulation and \emph{ggplot2} \cite{Wickham2009} for plotting. \textbf{Python} \cite{Foundation} was used to perform heavy computation, especially model fitting since the package for NLLS fitting in Python is much more robust than the one in R. Packages used: \emph{Pandas} \cite{McKinney2015} for using dataframes, \emph{NumPy} \cite{Â©Copyright2017NumPydevelopers.2017} for scientific and numeric computing, and \emph{LMFIT} \cite{Newville2014} for NLLS fitting. \textbf{\LaTeX} was used for typesetting. \textbf{Bash} was used to glue the R and Python scripts together. \textbf{Git} was used for version control of all codes/scripts/workflow.

	\section{Results \& Discussion}
	\subsection{Model Fitting}
	Model fitting convergence success rate was 100\% for 5 out of the 7 models used for NLLS fitting. The baranyi model converged 80.98\% of the time when fitted to all 305 groups, while the the logistic model did  not converge with over half the total number of groups in the dataset (44.92\%). We would generally expect to see all phenomenological models, as well as the buchanan model, to converge because of their low level of complexity.
	
	\par Figure \ref{tab:figure_68} below gives a general idea of what some of the data look like and how the models were able to be overlaid.
	
	\begin{figure}
		\centering
		\includegraphics[scale=0.5]{data_with_best_models_68.pdf}
		\caption{A general example of a bacterial population growth curve with all 7 models overlaid.}
		\label{tab:figure_68}
	\end{figure}

	\par However, visualizing the plots is not a reliable way of determining good fits. Looking at the model selection criteria can be more helpful in this case, which is shown in the next section.
	
	\subsection{Model Selection}

	\subsubsection{Case 1}
	In Figure \ref{tab:figure_52_146}, two plots are shown where the best model was deemed to be enough to predict the data. Looking at the model selection criteria in the dataset of both Figure \ref{tab:figure_52_146}(a) and \ref{tab:figure_52_146}(b), the gompertz model had the lowest AIC value, making it the best fit model relative to the others in the set. The AIC differences are all $>$ 10, meaning that they don't have any real effect in explaining the data in the presence of the gompertz model. This is confirmed by the likelihoods, akaike weights, and evidence ratios of all other models compared to gompertz.
	
	\begin{figure}
		\centering
		\subfloat[]{{\includegraphics[width=5cm, scale=0.5]{data_with_best_models_52.pdf}}}
		\qquad
		\subfloat[]{{\includegraphics[width=5cm, scale=0.5]{data_with_best_models_146.pdf}}}
		\caption{Population growth curve with 1 best model (gompertz in both cases)}
		\label{tab:figure_52_146}
	\end{figure}

	\subsubsection{Case 2}
	In Figure \ref{tab:figure_12}, the baranyi model outperforms the rest of them based on having the minimum AIC value. However, looking at the AIC differences, the gompertz model is very likely as well to be the best model to predict the data. The rest, however, have large AIC differences; therefore, they can be neglected. The evidence ratio of the gompertz model to the baranyi model is found to be 1.2398. That tells us there is a 1.2398/(1.2398+1) x 100 = 55.35\% chance that the baranyi model is the best fit for the data. That surely is not good enough; the baranyi model alone will not be a good enough predictor compared to when it is considered together with the gompertz model. Therefore, in such cases, both models should be considered to make biological inferences.
	
	\begin{figure}
		\centering
		\includegraphics[scale=0.5]{data_with_best_models_12.pdf}
		\caption{Population growth curve with 2 best models (in order of decreasing likelihood: baranyi, gompertz)}
		\label{tab:figure_12}
	\end{figure}

	\subsubsection{Case 3}
	In Figure \ref{tab:figure_242}, the plot may not be visually appealing, but it conveys the true objective of this project. The cubic model is considered to be the best one based on its relative AIC value. However, looking at the AIC differences, all, but one (logistic model), models are almost as likely to be the best model. The evidence ratios suggest there are 46.71\%, 43.73\%, 40.37\%, 33.78\%, 28.82\% chances for the quadratic, gompertz, baranyi, linear, or buchanan models to be the best fit for the data, respectively.

	\begin{figure}
		\centering
		\includegraphics[scale=0.5]{data_with_best_models_242.pdf}
		\caption{Population growth curve with 5 best models (in order of decreasing likelihood: logistic, gompertz, buchanan, baranyi, cubic)}
		\label{tab:figure_242}
	\end{figure}

	\section{Conclusion \& Future Work}
	Based on the different cases, one can make a clear argument about different ways of finding models to fit data. Sometimes, one model will truly be a good fit, so good that other models in the same set are incomparable. But, so often, especially in the world of biology, where natural phenomena happen randomly, one model will not be enough to capture the behavior of these phenomena. Therefore, multimodel inference, heavily based on information theory, should be considered as the tool for the future of modelling in general, and ecology in specific.
	
	\section{Acknowledgements}
	I would like to extend my gratitude to my fellow colleagues Pablo Lechon and Sam Turner for engaging in insightful discussions with me about the miniproject, as well as pulling an all-nighter the night before the submission deadline.
	
	\end{linenumbers}

	\bibliographystyle{unsrt}
    \bibliography{library}

\end{document}
